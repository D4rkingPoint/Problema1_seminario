{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e39ad6b",
   "metadata": {},
   "source": [
    "# Problema 1 - An√°lisis de Desinformaci√≥n en Redes Sociales en las Elecciones Presidenciales\n",
    "\n",
    "## *Autores*: \n",
    "- _Leonardo Ponce 202030531-5 (leonardo.ponde@usm.cl)_\n",
    "- _√Ålvaro Pozo 202030535-8 (alvaro.pozo@usm.cl)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726c3b2",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "En el marco de las elecciones presidenciales, las redes sociales han adquirido un rol\n",
    "central como espacio de difusi√≥n de informaci√≥n, debate y propaganda pol√≠tica. Sin\n",
    "embargo, tambi√©n se han convertido en terreno f√©rtil para la circulaci√≥n de noticias\n",
    "falsas (fake news), campa√±as de desinformaci√≥n coordinada y el uso de cuentas\n",
    "automatizadas (bots) que distorsionan la conversaci√≥n p√∫blica.\n",
    "Este problema busca que analicen c√≥mo se propaga este tipo de informaci√≥n, qu√©\n",
    "actores y comunidades la potencian, y c√≥mo es posible identificar patrones ocultos\n",
    "mediante el uso de redes, an√°lisis temporal y procesamiento de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90a1f1",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "En esta primera parte se trabajar√° con datos de redes sociales asociados a las elecciones presidenciales. El foco est√° en estudiar c√≥mo se propagan las noticias falsas en comparaci√≥n con publicaciones leg√≠timas.\n",
    "\n",
    "Tareas principales:\n",
    "1. Recolectar y organizar publicaciones relevantes sobre las elecciones (ej., con\n",
    "palabras clave o enlaces compartidos).\n",
    "2. Reconstruir cascadas de difusi√≥n (retuits, compartidos, menciones) como\n",
    "grafos de propagaci√≥n.\n",
    "3. Calcular m√©tricas de red (grado, betweenness, closeness, etc.) para identificar\n",
    "actores que amplifican rumores.\n",
    "4. Visualizar la din√°mica temporal y geogr√°fica de la propagaci√≥n.\n",
    "5. Distinguir posibles cuentas automatizadas a partir de sus patrones de actividad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb799787",
   "metadata": {},
   "source": [
    "# Desarrollo del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e825ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=== INICIANDO SCRAPING DE X ===\n",
      "INFO:__main__:Iniciando con 2 cuentas\n",
      "INFO:__main__:Query 1/6: fraude electoral\n",
      "INFO:__main__:Usando proxy gratuito: http://8.221.141.88:80\n",
      "ERROR:__main__:Error en login: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6f2c4e995+80021]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c4e9f0+80112]\n",
      "\t(No symbol) [0x0x7ff6f29d060f]\n",
      "\t(No symbol) [0x0x7ff6f2a28854]\n",
      "\t(No symbol) [0x0x7ff6f2a28b1c]\n",
      "\t(No symbol) [0x0x7ff6f2a7c927]\n",
      "\t(No symbol) [0x0x7ff6f2a5126f]\n",
      "\t(No symbol) [0x0x7ff6f2a7968a]\n",
      "\t(No symbol) [0x0x7ff6f2a51003]\n",
      "\t(No symbol) [0x0x7ff6f2a195d1]\n",
      "\t(No symbol) [0x0x7ff6f2a1a3f3]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f0dd4d+2960461]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f0800a+2936586]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f28a47+3070279]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c6847e+185214]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c6fecf+216527]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c57bd4+117460]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c57d8f+117903]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c3dc68+11112]\n",
      "\tBaseThreadInitThunk [0x0x7ff944b5e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ff945e8c53c+44]\n",
      "\n",
      "ERROR:__main__:Login fall√≥ para alvaro_gb01@hotmail.com\n",
      "INFO:__main__:Usando proxy gratuito: http://8.213.195.191:8080\n",
      "ERROR:__main__:Error en login: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=141.0.7390.54)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6f2c4e995+80021]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c4e9f0+80112]\n",
      "\t(No symbol) [0x0x7ff6f29d060f]\n",
      "\t(No symbol) [0x0x7ff6f29a82f1]\n",
      "\t(No symbol) [0x0x7ff6f2a588be]\n",
      "\t(No symbol) [0x0x7ff6f2a78fa2]\n",
      "\t(No symbol) [0x0x7ff6f2a51003]\n",
      "\t(No symbol) [0x0x7ff6f2a195d1]\n",
      "\t(No symbol) [0x0x7ff6f2a1a3f3]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f0dd4d+2960461]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f0800a+2936586]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f28a47+3070279]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c6847e+185214]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c6fecf+216527]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c57bd4+117460]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c57d8f+117903]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c3dc68+11112]\n",
      "\tBaseThreadInitThunk [0x0x7ff944b5e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ff945e8c53c+44]\n",
      "\n",
      "ERROR:__main__:Login fall√≥ para alvaro_gb01@hotmail.com\n",
      "INFO:__main__:Usando proxy gratuito: http://8.130.39.117:1000\n",
      "ERROR:__main__:Error en login: Message: invalid session id: session deleted as the browser has closed the connection\n",
      "from disconnected: not connected to DevTools\n",
      "  (Session info: chrome=141.0.7390.54); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6f2c4e995+80021]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c4e9f0+80112]\n",
      "\t(No symbol) [0x0x7ff6f29d060f]\n",
      "\t(No symbol) [0x0x7ff6f29bc145]\n",
      "\t(No symbol) [0x0x7ff6f29e177a]\n",
      "\t(No symbol) [0x0x7ff6f2a58b06]\n",
      "\t(No symbol) [0x0x7ff6f2a78fa2]\n",
      "\t(No symbol) [0x0x7ff6f2a51003]\n",
      "\t(No symbol) [0x0x7ff6f2a195d1]\n",
      "\t(No symbol) [0x0x7ff6f2a1a3f3]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f0dd4d+2960461]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f0800a+2936586]\n",
      "\tGetHandleVerifier [0x0x7ff6f2f28a47+3070279]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c6847e+185214]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c6fecf+216527]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c57bd4+117460]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c57d8f+117903]\n",
      "\tGetHandleVerifier [0x0x7ff6f2c3dc68+11112]\n",
      "\tBaseThreadInitThunk [0x0x7ff944b5e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ff945e8c53c+44]\n",
      "\n",
      "ERROR:__main__:Login fall√≥ para alvaro_gb01@hotmail.com\n",
      "INFO:__main__:Esperando 180 segundos...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "import itertools\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class XScraper:\n",
    "    def __init__(self, credentials_file=\"credentials.json\"):\n",
    "        self.credentials = self.load_credentials(credentials_file)\n",
    "        self.current_account_index = 0\n",
    "        self.proxy_cycle = itertools.cycle(self.credentials.get('proxies', [])) if self.credentials.get('proxies') else None\n",
    "        self.ua = UserAgent()\n",
    "        self.tweets_data = []\n",
    "        self.session_start_time = datetime.now()\n",
    "        self.tweets_scraped_this_session = 0\n",
    "        self.max_tweets_per_account = 800\n",
    "        \n",
    "    def load_credentials(self, file_path):\n",
    "        \"\"\"Carga credenciales desde archivo JSON\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Archivo de credenciales {file_path} no encontrado\")\n",
    "            return self.create_sample_credentials(file_path)\n",
    "    \n",
    "    def create_sample_credentials(self, file_path):\n",
    "        \"\"\"Crea archivo de credenciales de ejemplo\"\"\"\n",
    "        sample = {\n",
    "            \"accounts\": [\n",
    "                {\n",
    "                    \"username\": \"tu_usuario1\",\n",
    "                    \"password\": \"tu_contrase√±a1\",\n",
    "                    \"phone\": \"\"\n",
    "                }\n",
    "            ],\n",
    "            \"proxies\": [],\n",
    "            \"delays\": {\n",
    "                \"min_tweet_delay\": 2,\n",
    "                \"max_tweet_delay\": 5,\n",
    "                \"min_scroll_delay\": 3,\n",
    "                \"max_scroll_delay\": 7,\n",
    "                \"query_delay\": 180,\n",
    "                \"account_switch_delay\": 300\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sample, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Archivo de credenciales de ejemplo creado: {file_path}\")\n",
    "        return sample\n",
    "    \n",
    "    def get_free_proxies(self):\n",
    "        \"\"\"Obtiene proxies gratuitos\"\"\"\n",
    "        try:\n",
    "            # M√∫ltiples fuentes de proxies gratuitos\n",
    "            sources = [\n",
    "                \"https://api.proxyscrape.com/v2/?request=get&protocol=http&timeout=10000&country=all\",\n",
    "                \"https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt\"\n",
    "            ]\n",
    "            \n",
    "            all_proxies = []\n",
    "            for source in sources:\n",
    "                try:\n",
    "                    response = requests.get(source, timeout=10)\n",
    "                    proxies = response.text.strip().split('\\n')\n",
    "                    all_proxies.extend([f\"http://{proxy.strip()}\" for proxy in proxies if proxy.strip()])\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Validar algunos proxies\n",
    "            valid_proxies = []\n",
    "            for proxy in random.sample(all_proxies[:50], min(10, len(all_proxies))):\n",
    "                if self.test_proxy(proxy):\n",
    "                    valid_proxies.append(proxy)\n",
    "                    if len(valid_proxies) >= 5:  # M√°ximo 5 proxies v√°lidos\n",
    "                        break\n",
    "            \n",
    "            return valid_proxies\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error obteniendo proxies gratuitos: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def test_proxy(self, proxy):\n",
    "        \"\"\"Prueba si un proxy funciona\"\"\"\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                \"http://httpbin.org/ip\", \n",
    "                proxies={\"http\": proxy, \"https\": proxy},\n",
    "                timeout=5\n",
    "            )\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def setup_driver(self, use_proxy=True):\n",
    "        \"\"\"Configura el driver con medidas anti-detecci√≥n\"\"\"\n",
    "        chrome_options = Options()\n",
    "        \n",
    "        # Opciones b√°sicas anti-detecci√≥n\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_argument('--disable-extensions')\n",
    "        chrome_options.add_argument('--disable-plugins')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        chrome_options.add_argument('--disable-web-security')\n",
    "        chrome_options.add_argument('--allow-running-insecure-content')\n",
    "        \n",
    "        # User agent aleatorio\n",
    "        user_agent = self.ua.random\n",
    "        chrome_options.add_argument(f'--user-agent={user_agent}')\n",
    "        \n",
    "        # Configuraci√≥n de proxy\n",
    "        if use_proxy:\n",
    "            proxy = None\n",
    "            \n",
    "            # Usar proxies configurados\n",
    "            if self.proxy_cycle:\n",
    "                try:\n",
    "                    proxy = next(self.proxy_cycle)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Si no hay proxies configurados, obtener gratuitos\n",
    "            if not proxy and not self.credentials.get('proxies'):\n",
    "                free_proxies = self.get_free_proxies()\n",
    "                if free_proxies:\n",
    "                    proxy = random.choice(free_proxies)\n",
    "                    logger.info(f\"Usando proxy gratuito: {proxy}\")\n",
    "            \n",
    "            if proxy:\n",
    "                chrome_options.add_argument(f'--proxy-server={proxy}')\n",
    "                \n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            \n",
    "            # Scripts anti-detecci√≥n\n",
    "            driver.execute_script(\"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\n",
    "                window.chrome = { runtime: {} };\n",
    "                Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});\n",
    "                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});\n",
    "            \"\"\")\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except WebDriverException as e:\n",
    "            logger.error(f\"Error creando driver: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def human_delay(self, min_delay=None, max_delay=None):\n",
    "        \"\"\"Simula comportamiento humano con delays\"\"\"\n",
    "        delays = self.credentials.get('delays', {})\n",
    "        min_d = min_delay or delays.get('min_tweet_delay', 2)\n",
    "        max_d = max_delay or delays.get('max_tweet_delay', 5)\n",
    "        \n",
    "        delay = random.uniform(min_d, max_d)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def human_typing(self, element, text):\n",
    "        \"\"\"Simula tipeo humano\"\"\"\n",
    "        element.clear()\n",
    "        for char in text:\n",
    "            element.send_keys(char)\n",
    "            time.sleep(random.uniform(0.05, 0.2))\n",
    "    \n",
    "    def scroll_like_human(self, driver):\n",
    "        \"\"\"Simula scroll humano\"\"\"\n",
    "        # Scroll gradual\n",
    "        scroll_pause_time = random.uniform(1, 3)\n",
    "        \n",
    "        for _ in range(random.randint(2, 5)):\n",
    "            scroll_amount = random.randint(300, 800)\n",
    "            driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "        \n",
    "        # Ocasionalmente scroll hacia atr√°s\n",
    "        if random.random() < 0.2:\n",
    "            driver.execute_script(f\"window.scrollBy(0, -{random.randint(100, 300)});\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    def login_x(self, driver, account_info):\n",
    "        \"\"\"Login en X.com con manejo de verificaciones\"\"\"\n",
    "        try:\n",
    "            # Ir a la p√°gina de login de X\n",
    "            driver.get(\"https://x.com/i/flow/login\")\n",
    "            self.human_delay(3, 6)\n",
    "            \n",
    "            # Esperar y encontrar campo de usuario\n",
    "            username_input = WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'input[autocomplete=\"username\"]'))\n",
    "            )\n",
    "            \n",
    "            # Tipeo humano del username\n",
    "            self.human_typing(username_input, account_info['username'])\n",
    "            self.human_delay(1, 2)\n",
    "            \n",
    "            # Buscar y hacer clic en \"Next\"\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"Next\")]//ancestor::button | //button[contains(@role, \"button\") and contains(., \"Next\")]'))\n",
    "            )\n",
    "            next_button.click()\n",
    "            self.human_delay(2, 4)\n",
    "            \n",
    "            # Manejar verificaci√≥n de tel√©fono si aparece\n",
    "            try:\n",
    "                # Verificar si aparece el campo de verificaci√≥n\n",
    "                verification_input = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'input[data-testid=\"ocfEnterTextTextInput\"]'))\n",
    "                )\n",
    "                \n",
    "                if account_info.get('phone'):\n",
    "                    self.human_typing(verification_input, account_info['phone'])\n",
    "                    next_button = driver.find_element(By.XPATH, '//span[contains(text(), \"Next\")]//ancestor::button')\n",
    "                    next_button.click()\n",
    "                    self.human_delay(2, 4)\n",
    "                else:\n",
    "                    logger.warning(\"Se requiere verificaci√≥n de tel√©fono pero no est√° configurada\")\n",
    "                    return False\n",
    "                    \n",
    "            except TimeoutException:\n",
    "                # No hay verificaci√≥n de tel√©fono, continuar\n",
    "                pass\n",
    "            \n",
    "            # Campo de contrase√±a\n",
    "            password_input = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'input[name=\"password\"]'))\n",
    "            )\n",
    "            \n",
    "            self.human_typing(password_input, account_info['password'])\n",
    "            self.human_delay(1, 2)\n",
    "            \n",
    "            # Bot√≥n de login\n",
    "            login_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"Log in\")]//ancestor::button | //button[contains(., \"Log in\")]'))\n",
    "            )\n",
    "            login_button.click()\n",
    "            self.human_delay(5, 10)\n",
    "            \n",
    "            # Verificar login exitoso - buscar elementos caracter√≠sticos de X logueado\n",
    "            try:\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.any_of(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"SideNav_NewTweet_Button\"]')),\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '[aria-label=\"Home timeline\"]')),\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"primaryColumn\"]'))\n",
    "                    )\n",
    "                )\n",
    "                logger.info(f\"Login exitoso para {account_info['username']}\")\n",
    "                return True\n",
    "                \n",
    "            except TimeoutException:\n",
    "                logger.error(f\"Login fallido para {account_info['username']}\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en login: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_tweet_data(self, tweet_element):\n",
    "        \"\"\"Extrae datos completos de un tweet en X\"\"\"\n",
    "        try:\n",
    "            tweet_data = {}\n",
    "            \n",
    "            # ID del tweet\n",
    "            try:\n",
    "                tweet_links = tweet_element.find_elements(By.CSS_SELECTOR, 'a[href*=\"/status/\"]')\n",
    "                if tweet_links:\n",
    "                    tweet_url = tweet_links[0].get_attribute('href')\n",
    "                    tweet_id = tweet_url.split('/status/')[-1].split('?')[0]\n",
    "                    tweet_data['tweet_id'] = tweet_id\n",
    "                    tweet_data['url'] = tweet_url\n",
    "                else:\n",
    "                    return None\n",
    "            except:\n",
    "                return None\n",
    "            \n",
    "            # Texto del tweet\n",
    "            try:\n",
    "                text_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid=\"tweetText\"]')\n",
    "                tweet_data['text'] = text_element.text\n",
    "            except:\n",
    "                tweet_data['text'] = \"\"\n",
    "            \n",
    "            # Usuario\n",
    "            try:\n",
    "                user_elements = tweet_element.find_elements(By.CSS_SELECTOR, '[data-testid=\"User-Name\"] a')\n",
    "                if user_elements:\n",
    "                    username_href = user_elements[0].get_attribute('href')\n",
    "                    username = username_href.split('/')[-1] if username_href else \"\"\n",
    "                    tweet_data['username'] = username\n",
    "                else:\n",
    "                    tweet_data['username'] = \"\"\n",
    "            except:\n",
    "                tweet_data['username'] = \"\"\n",
    "            \n",
    "            # Nombre display\n",
    "            try:\n",
    "                display_elements = tweet_element.find_elements(By.CSS_SELECTOR, '[data-testid=\"User-Name\"] span')\n",
    "                if display_elements:\n",
    "                    tweet_data['display_name'] = display_elements[0].text\n",
    "                else:\n",
    "                    tweet_data['display_name'] = \"\"\n",
    "            except:\n",
    "                tweet_data['display_name'] = \"\"\n",
    "            \n",
    "            # Timestamp\n",
    "            try:\n",
    "                time_element = tweet_element.find_element(By.CSS_SELECTOR, 'time')\n",
    "                tweet_data['timestamp'] = time_element.get_attribute('datetime')\n",
    "            except:\n",
    "                tweet_data['timestamp'] = \"\"\n",
    "            \n",
    "            # M√©tricas (m√°s robustas)\n",
    "            metrics_map = {\n",
    "                'reply': 'replies',\n",
    "                'retweet': 'retweets', \n",
    "                'like': 'likes',\n",
    "                'bookmark': 'bookmarks'\n",
    "            }\n",
    "            \n",
    "            for test_id, key in metrics_map.items():\n",
    "                try:\n",
    "                    metric_elements = tweet_element.find_elements(By.CSS_SELECTOR, f'[data-testid=\"{test_id}\"]')\n",
    "                    if metric_elements:\n",
    "                        # Intentar obtener el n√∫mero del aria-label o texto\n",
    "                        metric_text = metric_elements[0].get_attribute('aria-label') or metric_elements[0].text\n",
    "                        # Extraer n√∫meros\n",
    "                        import re\n",
    "                        numbers = re.findall(r'[\\d,]+', metric_text)\n",
    "                        count = numbers[0].replace(',', '') if numbers else \"0\"\n",
    "                        tweet_data[key] = count\n",
    "                    else:\n",
    "                        tweet_data[key] = \"0\"\n",
    "                except:\n",
    "                    tweet_data[key] = \"0\"\n",
    "            \n",
    "            # Metadatos\n",
    "            tweet_data['scraped_at'] = datetime.now().isoformat()\n",
    "            tweet_data['scraper_account'] = self.credentials['accounts'][self.current_account_index]['username']\n",
    "            \n",
    "            return tweet_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error extrayendo datos del tweet: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_search_results(self, driver, query, max_tweets=1000):\n",
    "        \"\"\"Scraping de resultados de b√∫squeda en X\"\"\"\n",
    "        # URL de b√∫squeda en X\n",
    "        encoded_query = requests.utils.quote(query)\n",
    "        search_url = f\"https://x.com/search?q={encoded_query}&src=typed_query&f=live\"\n",
    "        \n",
    "        logger.info(f\"Buscando: {query}\")\n",
    "        driver.get(search_url)\n",
    "        self.human_delay(5, 10)\n",
    "        \n",
    "        tweets_found = 0\n",
    "        no_new_tweets_count = 0\n",
    "        max_no_new = 5\n",
    "        seen_tweet_ids = set()\n",
    "        \n",
    "        while tweets_found < max_tweets and no_new_tweets_count < max_no_new:\n",
    "            # Verificar l√≠mites\n",
    "            if self.tweets_scraped_this_session >= self.max_tweets_per_account:\n",
    "                logger.info(\"L√≠mite por cuenta alcanzado\")\n",
    "                break\n",
    "            \n",
    "            # Encontrar tweets\n",
    "            tweet_elements = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"tweet\"]')\n",
    "            \n",
    "            new_tweets_batch = 0\n",
    "            for tweet_element in tweet_elements:\n",
    "                try:\n",
    "                    tweet_data = self.extract_tweet_data(tweet_element)\n",
    "                    \n",
    "                    if (tweet_data and \n",
    "                        tweet_data.get('tweet_id') and \n",
    "                        tweet_data['tweet_id'] not in seen_tweet_ids):\n",
    "                        \n",
    "                        seen_tweet_ids.add(tweet_data['tweet_id'])\n",
    "                        self.tweets_data.append(tweet_data)\n",
    "                        tweets_found += 1\n",
    "                        new_tweets_batch += 1\n",
    "                        self.tweets_scraped_this_session += 1\n",
    "                        \n",
    "                        if tweets_found % 20 == 0:\n",
    "                            logger.info(f\"Tweets encontrados: {tweets_found}\")\n",
    "                        \n",
    "                        if tweets_found >= max_tweets:\n",
    "                            break\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Control de flujo\n",
    "            if new_tweets_batch == 0:\n",
    "                no_new_tweets_count += 1\n",
    "                logger.debug(f\"Sin tweets nuevos: {no_new_tweets_count}/{max_no_new}\")\n",
    "            else:\n",
    "                no_new_tweets_count = 0\n",
    "            \n",
    "            # Scroll humano\n",
    "            self.scroll_like_human(driver)\n",
    "            \n",
    "            # Delay entre scrolls\n",
    "            delays = self.credentials.get('delays', {})\n",
    "            self.human_delay(\n",
    "                delays.get('min_scroll_delay', 3),\n",
    "                delays.get('max_scroll_delay', 7)\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Completado '{query}': {tweets_found} tweets\")\n",
    "        return tweets_found\n",
    "\n",
    "def run_scraping_pipeline(search_queries, max_tweets_per_query=300):\n",
    "    \"\"\"Pipeline principal de scraping\"\"\"\n",
    "    scraper = XScraper()\n",
    "    \n",
    "    if not scraper.credentials.get('accounts'):\n",
    "        logger.error(\"No hay cuentas configuradas\")\n",
    "        return []\n",
    "    \n",
    "    logger.info(f\"Iniciando con {len(scraper.credentials['accounts'])} cuentas\")\n",
    "    \n",
    "    for query_idx, query in enumerate(search_queries):\n",
    "        logger.info(f\"Query {query_idx + 1}/{len(search_queries)}: {query}\")\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            driver = scraper.setup_driver()\n",
    "            if not driver:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Login\n",
    "                current_account = scraper.credentials['accounts'][scraper.current_account_index]\n",
    "                \n",
    "                if scraper.login_x(driver, current_account):\n",
    "                    # Scraping\n",
    "                    tweets_found = scraper.scrape_search_results(driver, query, max_tweets_per_query)\n",
    "                    logger.info(f\"Tweets recolectados para '{query}': {tweets_found}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logger.error(f\"Login fall√≥ para {current_account['username']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en intento {attempt + 1}: {e}\")\n",
    "                \n",
    "            finally:\n",
    "                if driver:\n",
    "                    driver.quit()\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)  # Esperar antes de reintentar\n",
    "        \n",
    "        # Rotaci√≥n de cuentas\n",
    "        if (query_idx + 1) % 2 == 0 and len(scraper.credentials['accounts']) > 1:\n",
    "            scraper.current_account_index = (scraper.current_account_index + 1) % len(scraper.credentials['accounts'])\n",
    "            scraper.tweets_scraped_this_session = 0\n",
    "            logger.info(f\"Cambiando a cuenta {scraper.current_account_index + 1}\")\n",
    "        \n",
    "        # Delay entre queries\n",
    "        if query_idx < len(search_queries) - 1:\n",
    "            delay = scraper.credentials.get('delays', {}).get('query_delay', 120)\n",
    "            logger.info(f\"Esperando {delay} segundos...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return scraper.tweets_data\n",
    "\n",
    "def save_results(tweets_data, filename=None):\n",
    "    \"\"\"Guarda resultados en m√∫ltiples formatos\"\"\"\n",
    "    if not tweets_data:\n",
    "        logger.warning(\"No hay datos para guardar\")\n",
    "        return None\n",
    "        \n",
    "    if not filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"x_scraping_{timestamp}\"\n",
    "    \n",
    "    df = pd.DataFrame(tweets_data)\n",
    "    \n",
    "    # Limpiar datos\n",
    "    df = df.drop_duplicates(subset=['tweet_id'])\n",
    "    \n",
    "    # Guardar en m√∫ltiples formatos\n",
    "    df.to_csv(f\"{filename}.csv\", index=False, encoding='utf-8-sig')\n",
    "    df.to_json(f\"{filename}.json\", orient='records', indent=2)\n",
    "    \n",
    "    # Excel con an√°lisis\n",
    "    with pd.ExcelWriter(f\"{filename}.xlsx\", engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Todos_los_tweets', index=False)\n",
    "        \n",
    "        # Resumen por usuario\n",
    "        if 'username' in df.columns and not df.empty:\n",
    "            user_stats = df.groupby('username').agg({\n",
    "                'text': 'count',\n",
    "                'likes': lambda x: pd.to_numeric(x, errors='coerce').sum(),\n",
    "                'retweets': lambda x: pd.to_numeric(x, errors='coerce').sum(),\n",
    "                'replies': lambda x: pd.to_numeric(x, errors='coerce').sum()\n",
    "            }).rename(columns={'text': 'tweet_count'})\n",
    "            user_stats = user_stats.sort_values('tweet_count', ascending=False)\n",
    "            user_stats.to_excel(writer, sheet_name='Resumen_usuarios')\n",
    "    \n",
    "    logger.info(f\"Datos guardados: {filename}.*\")\n",
    "    logger.info(f\"Total tweets √∫nicos: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Uso en Jupyter\n",
    "if __name__ == \"__main__\":\n",
    "    # T√©rminos de b√∫squeda para desinformaci√≥n electoral\n",
    "    search_queries = [\n",
    "        \"fraude electoral\",\n",
    "        \"elecciones manipuladas\",\n",
    "        \"voto fraudulento\",\n",
    "        \"urnas trucadas\", \n",
    "        \"conteo falso\",\n",
    "        \"elecciones robadas\"\n",
    "    ]\n",
    "    \n",
    "    # Ejecutar scraping\n",
    "    logger.info(\"=== INICIANDO SCRAPING DE X ===\")\n",
    "    tweets_data = run_scraping_pipeline(\n",
    "        search_queries=search_queries,\n",
    "        max_tweets_per_query=200\n",
    "    )\n",
    "    \n",
    "    # Guardar resultados\n",
    "    if tweets_data:\n",
    "        df_results = save_results(tweets_data)\n",
    "        \n",
    "        # Mostrar estad√≠sticas\n",
    "        print(f\"\\nüéØ SCRAPING COMPLETADO!\")\n",
    "        print(f\"üìä Total tweets: {len(tweets_data)}\")\n",
    "        if df_results is not None and not df_results.empty:\n",
    "            print(f\"üë• Usuarios √∫nicos: {df_results['username'].nunique()}\")\n",
    "            print(f\"üìÖ Per√≠odo: {df_results['timestamp'].min()} - {df_results['timestamp'].max()}\")\n",
    "            \n",
    "            # Muestra de datos\n",
    "            print(f\"\\nüìù Muestra de tweets:\")\n",
    "            print(df_results[['username', 'text', 'likes', 'retweets']].head())\n",
    "    else:\n",
    "        logger.error(\"‚ùå No se recolectaron datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eee011",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscrape_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.threads.net/t/C8H5FiCtESk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py:191\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug, loop_factory)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "import jmespath\n",
    "from nested_lookup import nested_lookup\n",
    "from scrapfly import ScrapflyClient, ScrapeConfig\n",
    "\n",
    "SCRAPFLY = ScrapflyClient(key=\"YOUR SCRAPFLY KEY\")\n",
    "\n",
    "\n",
    "def parse_thread(data: Dict) -> Dict:\n",
    "    \"\"\"Parse Twitter tweet JSON dataset for the most important fields\"\"\"\n",
    "    result = jmespath.search(\n",
    "        \"\"\"{\n",
    "        text: post.caption.text,\n",
    "        published_on: post.taken_at,\n",
    "        id: post.id,\n",
    "        pk: post.pk,\n",
    "        code: post.code,\n",
    "        username: post.user.username,\n",
    "        user_pic: post.user.profile_pic_url,\n",
    "        user_verified: post.user.is_verified,\n",
    "        user_pk: post.user.pk,\n",
    "        user_id: post.user.id,\n",
    "        has_audio: post.has_audio,\n",
    "        reply_count: post.text_post_app_info.direct_reply_count,\n",
    "        like_count: post.like_count,\n",
    "        images: post.carousel_media[].image_versions2.candidates[1].url,\n",
    "        image_count: post.carousel_media_count,\n",
    "        videos: post.video_versions[].url\n",
    "    }\"\"\",\n",
    "        data,\n",
    "    )\n",
    "    result[\"videos\"] = list(set(result[\"videos\"] or []))\n",
    "    if result[\"reply_count\"] and type(result[\"reply_count\"]) != int:\n",
    "        result[\"reply_count\"] = int(result[\"reply_count\"].split(\" \")[0])\n",
    "    result[\n",
    "        \"url\"\n",
    "    ] = f\"https://www.threads.net/@{result['username']}/post/{result['code']}\"\n",
    "    return result\n",
    "\n",
    "\n",
    "async def scrape_thread(url: str) -> dict:\n",
    "    \"\"\"Scrape Threads post and replies from a given URL\"\"\"\n",
    "    _xhr_calls = []\n",
    "    result = await SCRAPFLY.async_scrape(\n",
    "        ScrapeConfig(\n",
    "            url,\n",
    "            asp=True,  # enables scraper blocking bypass if any\n",
    "            country=\"US\",  # use US IP address as threads is only available in select countries\n",
    "        )\n",
    "    )\n",
    "    hidden_datasets = result.selector.css(\n",
    "        'script[type=\"application/json\"][data-sjs]::text'\n",
    "    ).getall()\n",
    "    # find datasets that contain threads data\n",
    "    for hidden_dataset in hidden_datasets:\n",
    "        # skip loading datasets that clearly don't contain threads data\n",
    "        if '\"ScheduledServerJS\"' not in hidden_dataset:\n",
    "            continue\n",
    "        if \"thread_items\" not in hidden_dataset:\n",
    "            continue\n",
    "        data = json.loads(hidden_dataset)\n",
    "        # datasets are heavily nested, use nested_lookup to find\n",
    "        # the thread_items key for thread data\n",
    "        thread_items = nested_lookup(\"thread_items\", data)\n",
    "        if not thread_items:\n",
    "            continue\n",
    "        # use our jmespath parser to reduce the dataset to the most important fields\n",
    "        threads = [parse_thread(t) for thread in thread_items for t in thread]\n",
    "        return {\n",
    "            \"thread\": threads[0],\n",
    "            \"replies\": threads[1:],\n",
    "        }\n",
    "    raise ValueError(\"could not find thread data in page\")\n",
    "\n",
    "\n",
    "# Example use:\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    print(asyncio.run(scrape_thread(\"https://www.threads.net/t/C8H5FiCtESk\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212afa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f8f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249191f7",
   "metadata": {},
   "source": [
    "## Las celdas deben estar comentadas!\n",
    "## Deben haber graficos!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fe24e",
   "metadata": {},
   "source": [
    "# Preguntas orientadoras:\n",
    "- ¬øQu√© candidatos aparecen m√°s vinculados a comunidades de desinformaci√≥n?\n",
    "- ¬øQu√© t√©rminos o narrativas destacan en las comunidades y qu√© sesgos reflejan?\n",
    "- ¬øQu√© patrones de conexi√≥n entre comunidades ayudan a explicar la propagaci√≥n de narrativas falsas?\n",
    "- ¬øQu√© diferencias se observan entre comunidades dominadas por humanos y aquellas potenciadas por bots?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239f1ec",
   "metadata": {},
   "source": [
    "# Discusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914943a",
   "metadata": {},
   "source": [
    "***La discusi√≥n debe estar conectada con las preguntas orientadoras***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
